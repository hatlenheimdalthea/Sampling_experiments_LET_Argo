{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7f99c25-b235-47ac-95da-56e6187a8168",
   "metadata": {},
   "source": [
    "This notebook makes a dataframe for inputs (target and feature variables) from the Large Ensemble Testbed (LET) based on SOCAT and Argo observations and then reconstructs pCO2-Residual globally using XGB. See Zenodo link in the repository to access the sampling masks. \n",
    "\n",
    "First, the direct effect of temperature is removed from pCO2 (i.e., pCO2-T), giving the resulting pCO2-Residual which is used for the reconstruction. The code for this calculation is found in a different notebook in the repository (\"calculate_pco2_components\"). \n",
    "\n",
    "Section I:\n",
    "Creates a dataframe for inputs (target and feature variables) from the Large Ensemble Testbed for the XGB algorithm. \n",
    "\n",
    "Section II:\n",
    "Then, the XGBoost algorithm reconstructs the surface ocean pCO2-Residual globally over 1982-2016. \n",
    "\n",
    "Section III:\n",
    "Finally, pCO2-T is added back to the pCO2-Residual to get pCO2.  \n",
    "\n",
    "This notebook was originally created by Val Bennington, and modified by Thea Hatlen Heimdal and Devan Samant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7f2e373-7fbd-4ef8-aa6f-03a7298d999f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard imports\n",
    "import os\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import scipy\n",
    "import random\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import pickle\n",
    "\n",
    "# machine learning libraries\n",
    "import sklearn            # machine-learning libary with many algorithms implemented\n",
    "import xgboost as xgb     # extreme gradient boosting (XGB)\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Python file with supporting functions\n",
    "import pre_argo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a696d21-8db0-492c-86e5-4366d55bc802",
   "metadata": {},
   "source": [
    "### <font color='red'>Section I</font> \n",
    "#### Make dataframe for inputs for XGB algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "894053ba-e0b1-4556-a711-fcc27f3dddb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accessing directories\n",
    "\n",
    "root_dir = \"/data/artemis/workspace/theimdal/GO-BGC\"\n",
    "\n",
    "# Path to the dataframe created in this notebook\n",
    "data_output_dir = f\"{root_dir}/LET_selected_XGB/500_floats_optimized\"\n",
    "\n",
    "# Path to pickle file with list of LET members\n",
    "reference_output_dir = \"/data/artemis/workspace/theimdal/saildrone/LET_pickle_files\"\n",
    "\n",
    "# Path to \"raw\" LET output (testbed truth)\n",
    "ensemble_dir_head = \"/local/data/artemis/simulations/LET\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe5bbb56-6c21-4358-bb9e-161f2c3ad5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading references to \"pick\" members from the LET\n",
    "# We use 75 members in total, 25 each from CESM, CanESM2 and GFDL\n",
    "# \"mems_dict\" is a list of the members of the LET\n",
    "\n",
    "path_LET = f\"{reference_output_dir}/members_LET_dict.pickle\"\n",
    "path_seeds = f\"{reference_output_dir}/random_seeds.npy\"\n",
    "path_loc = f\"{reference_output_dir}/members_seed_loc_dict.pickle\"\n",
    "with open(path_LET,'rb') as handle:\n",
    "    mems_dict = pickle.load(handle)\n",
    "    random_seeds = np.load(path_seeds)\n",
    "    with open(path_loc,'rb') as handle:\n",
    "        seed_loc_dict = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a35a989f-5fe5-4cf6-a6c2-208bb4e630ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CESM': ['001',\n",
       "  '002',\n",
       "  '009',\n",
       "  '010',\n",
       "  '011',\n",
       "  '012',\n",
       "  '013',\n",
       "  '014',\n",
       "  '015',\n",
       "  '016',\n",
       "  '017',\n",
       "  '018',\n",
       "  '020',\n",
       "  '021',\n",
       "  '023',\n",
       "  '024',\n",
       "  '025',\n",
       "  '030',\n",
       "  '031',\n",
       "  '034',\n",
       "  '035',\n",
       "  '101',\n",
       "  '102',\n",
       "  '103',\n",
       "  '104'],\n",
       " 'GFDL': ['01',\n",
       "  '02',\n",
       "  '03',\n",
       "  '04',\n",
       "  '05',\n",
       "  '06',\n",
       "  '08',\n",
       "  '09',\n",
       "  '10',\n",
       "  '11',\n",
       "  '12',\n",
       "  '13',\n",
       "  '14',\n",
       "  '16',\n",
       "  '17',\n",
       "  '18',\n",
       "  '19',\n",
       "  '20',\n",
       "  '22',\n",
       "  '23',\n",
       "  '26',\n",
       "  '27',\n",
       "  '28',\n",
       "  '29',\n",
       "  '30'],\n",
       " 'CanESM2': ['r1r10',\n",
       "  'r1r9',\n",
       "  'r3r1',\n",
       "  'r4r5',\n",
       "  'r5r10',\n",
       "  'r2r1',\n",
       "  'r3r2',\n",
       "  'r3r9',\n",
       "  'r4r6',\n",
       "  'r5r2',\n",
       "  'r1r6',\n",
       "  'r2r2',\n",
       "  'r3r4',\n",
       "  'r4r1',\n",
       "  'r4r7',\n",
       "  'r5r4',\n",
       "  'r1r7',\n",
       "  'r3r6',\n",
       "  'r4r8',\n",
       "  'r5r5',\n",
       "  'r2r8',\n",
       "  'r3r7',\n",
       "  'r4r3',\n",
       "  'r5r1',\n",
       "  'r5r9']}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mems_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5856502-83d8-4dfa-a83b-681ee393ead6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the date range to unify the date type\n",
    "# The LET output ranges from 1982-2017\n",
    "\n",
    "# Define date range\n",
    "date_range_start = '1982-01-01T00:00:00.000000000'\n",
    "date_range_end = '2017-01-01T00:00:00.000000000'\n",
    "\n",
    "# create date vector\n",
    "dates = pd.date_range(start=date_range_start, \n",
    "                      end=date_range_end,freq='MS') + np.timedelta64(14, 'D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfe0c13-8253-4dee-bdc2-e9c4fa249e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"pre_argo\" is a python file with supporting functions\n",
    "# This creates a dataframe with all inputs (features and target variables) needed for the machine learning\n",
    "\n",
    "for ens, mem_list in mems_dict.items(): \n",
    "    print(ens)\n",
    "    for member in mem_list:\n",
    "        print(member)\n",
    "        \n",
    "        df = pre_argo.create_inputs(ensemble_dir_head, ens, member, dates)\n",
    "    \n",
    "        # Save the data frame \n",
    "        pre_argo.save_clean_data(df, data_output_dir, ens, member)\n",
    "        #del df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be1f97a-4983-44c2-8627-da044ad47cf6",
   "metadata": {},
   "source": [
    "### <font color='red'>Section II</font> \n",
    "#### For Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99872216-95f2-4263-9aa4-544b29b7fd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accessing directories\n",
    "\n",
    "root_dir = \"/local/data/artemis/workspace/theimdal/GO-BGC\"\n",
    "\n",
    "data_output_dir = f\"{root_dir}/LET_selected_XGB/500_floats_optimized\"\n",
    "\n",
    "model_output_dir = f\"{root_dir}/models/500_floats_optimized/bias4/trained\"\n",
    "recon_output_dir = f\"{root_dir}/models/500_floats_optimized/bias4/reconstructions\"\n",
    "other_output_dir = f\"{root_dir}/models/500_floats_optimized/bias4/performance_metrics\"\n",
    "\n",
    "approach = 'xg'\n",
    "approach_output_dir = f\"{other_output_dir}/{approach}\"\n",
    "\n",
    "reference_output_dir = \"/data/artemis/workspace/theimdal/saildrone/LET_pickle_files\"\n",
    "\n",
    "# Number of cores you have access to for model training\n",
    "\n",
    "jobs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb11109-aceb-4670-a57a-41afdfb6962b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading references\n",
    "path_LET = f\"{reference_output_dir}/members_LET_dict.pickle\"\n",
    "path_seeds = f\"{reference_output_dir}/random_seeds.npy\"\n",
    "path_loc = f\"{reference_output_dir}/members_seed_loc_dict.pickle\"\n",
    "\n",
    "with open(path_LET,'rb') as handle:\n",
    "    mems_dict = pickle.load(handle)\n",
    "    \n",
    "random_seeds = np.load(path_seeds)    \n",
    "    \n",
    "with open(path_loc,'rb') as handle:\n",
    "    seed_loc_dict = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cbd7c085-1b0f-40a5-9de8-9875d22aed00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'CESM': {'max_depth': 6, 'n_estimators': 4000}, 'GFDL': {'max_depth': 6, 'n_estimators': 4000}, 'MPI': {'max_depth': 6, 'n_estimators': 4000}, 'CanESM2': {'max_depth': 6, 'n_estimators': 4000}}\n"
     ]
    }
   ],
   "source": [
    "# Load Best Parameters from optimization from previous pCO2-Residual Run\n",
    "# These parameters were held constant for all sampling experiments so that we could strictly \n",
    "# assess the impact of the floats\n",
    "\n",
    "path_bp=\"/data/artemis/workspace/vbennington/full_sst/pCO2_DIC/models/performance_metrics/xg/xg_best_params_dict.pickle\"\n",
    "with open(path_bp,'rb') as handle:\n",
    "    best_params = pickle.load(handle)\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175cb5a6-4958-4d55-8b8f-6b446c12d148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining inputs for the modeling process\n",
    "\n",
    "# Train-validate-test split proportions\n",
    "val_prop = .2\n",
    "test_prop = .2\n",
    "\n",
    "# Parameter grids\n",
    "xg_param_grid = {\"n_estimators\":[2000, 3000, 4000],\n",
    "                 \"max_depth\":[4, 5, 6]\n",
    "                }\n",
    "# Feature and target lists for feeding into the models\n",
    "features_sel = ['sst','sst_anom','sss','sss_anom','mld_clim_log','chl_log','chl_log_anom','xco2','A', 'B', 'C', 'T0', 'T1'] \n",
    "\n",
    "# What we reconstruct: pCO2-Residual = pCO2 - pCO2_T\n",
    "target_sel = ['pCO2_pCO2T_diff'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21119c7-eecd-405a-bd61-1c2bca39cdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to train with 4 out of every 5 months, and test on the fifth month\n",
    "# so that we don't train and test in the same month\n",
    "\n",
    "date_range_start = '1979-01-01T00:00:00.000000000'\n",
    "date_range_end = '2017-01-01T00:00:00.000000000'\n",
    "\n",
    "# Create date vector\n",
    "dates = pd.date_range(start=date_range_start, \n",
    "                      end=date_range_end,freq='MS') + np.timedelta64(14, 'D')\n",
    "select_dates = []\n",
    "test_dates = []\n",
    "for i in range(0,len(dates)):\n",
    "    if i % 5 != 0:\n",
    "        select_dates.append(dates[i])\n",
    "    if i % 5 == 0:\n",
    "        test_dates.append(dates[i])\n",
    "year_mon = []\n",
    "for i in range(0,len(select_dates)):\n",
    "    tmp = select_dates[i]\n",
    "    year_mon.append(f\"{tmp.year}-{tmp.month}\")\n",
    "test_year_mon = []\n",
    "for i in range(0,len(test_dates)):\n",
    "    tmp = test_dates[i]\n",
    "    test_year_mon.append(f\"{tmp.year}-{tmp.month}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d763ac-69c4-4533-8458-9a5b5d81b473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make dictionary to \"store\" performance metrics for the reconstruction\n",
    "# \"Unseen\" = every 1x1 grid cell that is not in the train or test set.\n",
    "# Since we are using a model testbed and not real-world observations we have unseen output \n",
    "test_performance = defaultdict(dict)\n",
    "unseen_performance = defaultdict(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fbe621-c8c8-4f28-87ac-3624f1435e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "K_folds = 3\n",
    "approach = \"xg\"\n",
    "first_mem = False # Switch to True if using gridsearch to find best_params\n",
    "\n",
    "print(datetime.datetime.now())\n",
    "for ens, mem_list in mems_dict.items():\n",
    "    \n",
    "    for member in mem_list:\n",
    "        print(ens)\n",
    "        print(member)\n",
    "        seed_loc = seed_loc_dict[ens][member] \n",
    "        \n",
    "        \n",
    "        # Data file path\n",
    "        data_dir = f\"{data_output_dir}/{ens}/member_{member}\"\n",
    "        fname = f\"data_clean_2D_mon_{ens}_{member}_1x1_198201-201701.pkl\"\n",
    "        file_path = f\"{data_dir}/{fname}\"\n",
    "        \n",
    "        # Read in data, create some selection filters, produce a reduced dataframe\n",
    "        df = pd.read_pickle(file_path)\n",
    "        \n",
    "        # =====================================================================\n",
    "        ### THIS CODE IS ONLY NEEDED WHEN ADDING UNCERTAINTY OR BIAS OF FLOATS\n",
    "        # =====================================================================\n",
    "              \n",
    "        #add random error to every float observation\n",
    "        df['random_number'] = np.random.uniform(-11,11, len(df))\n",
    "        df['pCO2_pCO2T_diff_new'] = np.where(df.argo_mask == 1, df.pCO2_pCO2T_diff+df.random_number, df.pCO2_pCO2T_diff)\n",
    "        \n",
    "        ################ OR ################ \n",
    "        \n",
    "        #add +4 uatm to every float observation\n",
    "        df['pCO2_pCO2T_diff_new'] = np.where(df.argo_mask == 1, df.pCO2_pCO2T_diff+4, df.pCO2_pCO2T_diff) \n",
    "        \n",
    "        #check the dataframe (adding bias/uncertainty)\n",
    "        display(df[df.socat_mask==1])\n",
    "        assert 1==0\n",
    "        \n",
    "        # =====================================================================\n",
    "        ### THIS CODE IS ONLY NEEDED WHEN ADDING UNCERTAINTY OR BIAS OF FLOATS\n",
    "        # =====================================================================\n",
    "        \n",
    "        df['year'] = df.index.get_level_values('time').year\n",
    "        df['mon'] = df.index.get_level_values('time').month\n",
    "        df['year_month'] = df['year'].astype(str) + \"-\" + df['mon'].astype(str)\n",
    "        \n",
    "  \n",
    "        #For pCO2-Residual:\n",
    "        # Apply mask (remove coast, shallow seas, Arctic etc.) and remove pCO2 values between -250 and +250 ppm\n",
    "        recon_sel = (~df[features_sel+target_sel+['net_mask']].isna().any(axis=1)) & ((df[target_sel] < 250) & (df[target_sel] > -250)).to_numpy().ravel()\n",
    "       \n",
    "        # Select 1x1 grid cells that are not masked as above, and that represents SOCAT and Argo sampling\n",
    "        sel = (recon_sel & ((df['socat_mask'] == 1) | (df['argo_mask']==1)))  \n",
    "      \n",
    "        # Select training set:\n",
    "        # Need to train where there is SOCAT/Argo data and during a non-test year\n",
    "        train_sel = (sel & (pd.Series(df['year_month']).isin(year_mon))).to_numpy().ravel()\n",
    "        print(sum(train_sel))\n",
    "    \n",
    "        # Select test set, not during a training month\n",
    "        test_sel = (sel & (pd.Series(df['year_month']).isin(test_year_mon))).to_numpy().ravel()\n",
    "        print(sum(test_sel))\n",
    "    \n",
    "        # Select \"unseen\" data - all 1x1 grid cells that are NOT in the train or test set\n",
    "        unseen_sel = (recon_sel & (df['socat_mask'] == 0) & (np.isnan(df['argo_mask']))) # locations not masked, not ridiculous pCO2, \n",
    "\n",
    "        ################################################################################################################################\n",
    "        \n",
    "        # Convert dataframe to numpy arrays, train/val/test split\n",
    "        X = df.loc[sel,features_sel].to_numpy()         \n",
    "        y = df.loc[sel,target_sel].to_numpy().ravel()\n",
    "        \n",
    "        # Convert dataframe to numpy arrays, train/val/test split\n",
    "        Xtrain = df.loc[train_sel,features_sel].to_numpy() # create Xtrain and Xtest to randomly select from for X_train and X_test\n",
    "        ytrain = df.loc[train_sel,target_sel].to_numpy().ravel()\n",
    "        \n",
    "        N = Xtrain.shape[0]\n",
    "        train_val_idx, train_idx, val_idx, test_idx = pre_argo.train_val_test_split(N, test_prop, val_prop, random_seeds, seed_loc)\n",
    "        X_train_val, X_train, X_val, X_test_tmp, y_train_val, y_train, y_val, y_test_tmp = pre_argo.apply_splits(Xtrain, ytrain, train_val_idx, train_idx, val_idx, test_idx) \n",
    "        \n",
    "        # We don't use X_test_tmp or y_test_tmp when test years are used (X_test and y_test set above)\n",
    "        X_test = df.loc[test_sel,features_sel].to_numpy()                #  Test metrics on all of SOCAT data from test years\n",
    "        y_test = df.loc[test_sel,target_sel].to_numpy().ravel()    \n",
    "        \n",
    "        # Define the model based on which approach to use    \n",
    "        if first_mem:\n",
    "            model = XGBRegressor(random_state=random_seeds[4,seed_loc], n_jobs=jobs)\n",
    "            param_grid = xg_param_grid\n",
    "            grid = GridSearchCV(model, param_grid, scoring='neg_mean_squared_error', cv=K_folds, return_train_score=False, refit=False)\n",
    "            grid.fit(X_train_val, y_train_val)\n",
    "            best_params[ens] = grid.best_params_\n",
    "            print(best_params)\n",
    "            first_mem = False\n",
    "\n",
    "        # Fit the model on train/validation data\n",
    "        model = XGBRegressor(random_state=random_seeds[5,seed_loc], **best_params[ens], n_jobs=jobs)\n",
    "        model.fit(X_train_val, y_train_val)          \n",
    "\n",
    "        # Save the model\n",
    "        pre_argo.save_model(model, model_output_dir, approach, ens, member)   \n",
    "\n",
    "        # Calculate test error metrics and store in a dictionary\n",
    "        y_pred_test = model.predict(X_test)\n",
    "        test_performance[ens][member] = pre_argo.evaluate_test(y_test, y_pred_test)\n",
    "        print(test_performance[ens][member])\n",
    "        \n",
    "        # Redo this analysis on the unseen data\n",
    "        y_pred_unseen = model.predict(df.loc[unseen_sel,features_sel].to_numpy())\n",
    "        y_unseen = df.loc[unseen_sel,target_sel].to_numpy().ravel()\n",
    "        unseen_performance[ens][member] = pre_argo.evaluate_test(y_unseen, y_pred_unseen)\n",
    "        print(unseen_performance[ens][member])\n",
    "\n",
    "        # Create the reconstruction and save it\n",
    "        # \"seen\": All SOCAT and USV locations for all training years (not test years)\n",
    "        y_pred_seen = model.predict(X)\n",
    "        \n",
    "        # Full reconstruction (all 1x1 degree grid cells)\n",
    "        df['pCO2_DIC_recon'] = np.nan\n",
    "        df.loc[unseen_sel,['pCO2_DIC_recon']] = y_pred_unseen   # Not in a SOCAT location, not even in test year\n",
    "        df.loc[sel,['pCO2_DIC_recon']] = y_pred_seen\n",
    "        \n",
    "        # All time/locations not sampled by SOCAT and Argo (\"unseen\")\n",
    "        df['pCO2_DIC_nosocat'] = np.nan\n",
    "        df.loc[unseen_sel,['pCO2_DIC_nosocat']] = y_pred_unseen\n",
    "        df.loc[sel,['pCO2_DIC_nosocat']] = np.nan\n",
    "        \n",
    "        # Only at time/locations of SOCAT and Argo sampling (\"training data\")\n",
    "        df['pCO2_DIC_socat'] = np.nan\n",
    "        df.loc[unseen_sel,['pCO2_DIC_socat']] = np.nan\n",
    "        df.loc[sel,['pCO2_DIC_socat']] = y_pred_seen\n",
    "     \n",
    "        df['pCO2_DIC'] = df['pCO2_pCO2T_diff']\n",
    "        \n",
    "        # =========================================\n",
    "        ### DESCRIPTION OF SAVED DATA SETS\n",
    "        # =========================================\n",
    "        \n",
    "        #pCO2_DIC = pCO2-Residual\n",
    "        #pCO2_DIC_recon = FULL reconstruction\n",
    "        #pCO2_DIC_socat = training data (also including Argo locations)\n",
    "        #pCO2_DIC_nosocat = UNSEEN reconstruction\n",
    "             \n",
    "        DS_recon = df[['net_mask','socat_mask','pCO2_DIC','pCO2_DIC_recon','pCO2_DIC_socat','pCO2_DIC_nosocat']].to_xarray()\n",
    "             \n",
    "        pre_argo.save_recon(DS_recon, recon_output_dir, approach, ens, member)  \n",
    "\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2190e93-25ef-480b-93a9-f3e7d24d8cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving performance metrics (and best parameters if grid search was carried out)\n",
    "\n",
    "approach_output_dir = f\"{other_output_dir}/{approach}\"\n",
    "param_fname = f\"{approach_output_dir}/{approach}_best_params_dict.pickle\"\n",
    "test_perform_fname = f\"{approach_output_dir}/{approach}_test_performance_dict.pickle\"\n",
    "unseen_perform_fname = f\"{approach_output_dir}/{approach}_unseen_performance_dict.pickle\"\n",
    "\n",
    "Path(approach_output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(param_fname, 'wb') as handle:\n",
    "    pickle.dump(best_params, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open(test_perform_fname, 'wb') as handle:\n",
    "    pickle.dump(test_performance, handle)\n",
    "with open(unseen_perform_fname, 'wb') as handle:\n",
    "    pickle.dump(unseen_performance, handle)\n",
    "    \n",
    "# Convert performance metrics to dataframes\n",
    "test_df = pd.DataFrame.from_dict({(i,j): test_performance[i][j]\n",
    "                                  for i in test_performance.keys()\n",
    "                                  for j in test_performance[i].keys()},\n",
    "                                 orient='index')\n",
    "\n",
    "unseen_df = pd.DataFrame.from_dict({(i,j): unseen_performance[i][j]\n",
    "                                  for i in unseen_performance.keys()\n",
    "                                  for j in unseen_performance[i].keys()},\n",
    "                                 orient='index')\n",
    "\n",
    "test_df.index.names = [\"model\",\"member\"]\n",
    "unseen_df.index.names = [\"model\",\"member\"]\n",
    "\n",
    "# Save the dataframes too\n",
    "test_df_fname = f\"{approach_output_dir}/{approach}_test_performance_df.pickle\"\n",
    "unseen_df_fname = f\"{approach_output_dir}/{approach}_unseen_performance_df.pickle\"\n",
    "\n",
    "test_df.to_pickle(test_df_fname)\n",
    "unseen_df.to_pickle(unseen_df_fname)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8a8634-d4b0-498e-a357-2dfd696af6d2",
   "metadata": {},
   "source": [
    "### <font color='red'>Section III</font> \n",
    "#### Calculate pCO2 from the pCO2-Residual reconstructions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bbeb5d-f401-4253-8116-c461028eb681",
   "metadata": {},
   "source": [
    "The pCO2-Residual is reconstructed in space and time. pCO2-T is now added back to pCO2-Residual to get pCO2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbe7451-9d5f-422a-b904-17387d8a6a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pCO2-Residual reconstruction\n",
    "run_dir = \"/data/artemis/workspace/theimdal/GO-BGC/models/500_floats/uncertainty11/reconstructions/xg\"\n",
    "\n",
    "# Loading references to \"pick\" members from the LET\n",
    "# We use 75 members in total, 25 each from CESM, CanESM2 and GFDL\n",
    "# \"mems_dict\" is a list of the members of the LET\n",
    "reference_output_dir = \"/data/artemis/workspace/theimdal/saildrone/LET_pickle_files\"\n",
    "path_LET = f\"{reference_output_dir}/members_LET_dict.pickle\"\n",
    "\n",
    "with open(path_LET,'rb') as handle:\n",
    "    mems_dict = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a54f6c6-b8f4-4d06-9c3b-bd09b7d6e545",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ens, mem_list in mems_dict.items():\n",
    "    print(ens)\n",
    "    #if ens ==\"CESM\":\n",
    "        #mem_list = []\n",
    "        \n",
    "    for member in mem_list:\n",
    "        \n",
    "        member_dir = f\"{run_dir}/{ens}/member_{member}\" #run_dir = XGB reconstructions\n",
    "        \n",
    "        #pCO2-Residual reconstruction\n",
    "        pco2D_path = f\"{member_dir}/xg_recon_pC02_2D_mon_{ens}_{member}_1x1_198201-201701.nc\" \n",
    "        \n",
    "        # pCO2-T in this file \n",
    "        pco2T_path = f\"/data/artemis/workspace/vbennington/LET/{ens}/pCO2_components_{ens}{member}_198201-201701.nc\"  \n",
    "        \n",
    "        # Path for outputting pCO2 (pCO2-T added back to pCO2-Residual)\n",
    "        file_out = f\"{member_dir}/recon_pCO2DIC_pCO2_2D_mon_{ens}_{member}_1x1_198201-201701.nc\"\n",
    "       \n",
    "        if ens==\"CanESM2\":\n",
    "            pco2T_path = f\"/data/artemis/workspace/vbennington/LET/{ens}/pCO2_components_{ens}{member}_198201-201712.nc\"  # pCO2-T in this file,\n",
    "            \n",
    "        # Load modeled pCO2-T and reconstructed pCO2-DIC:\n",
    "        pco2T_series = xr.open_dataset(pco2T_path).pCO2_T.transpose(\"time\",\"ylat\",\"xlon\") # pCO2-T calculated from model pCO2 and SST\n",
    "\n",
    "        pco2D_series = xr.open_dataset(pco2D_path).pCO2_DIC_recon.transpose(\"time\",\"ylat\",\"xlon\") # Reconstructed pCO2-Residual from XGB\n",
    "        \n",
    "        pco2D_unseen_series = xr.open_dataset(pco2D_path).pCO2_DIC_nosocat.transpose(\"time\",\"ylat\",\"xlon\") # Reconstructed pCO2-Residual from XGB\n",
    "        \n",
    "        \n",
    "        # Get time coordinate correct\n",
    "        pco2T_series = pco2T_series.assign_coords({\"time\":(\"time\",pco2D_series.time)})\n",
    "        \n",
    "        pco2_series = pco2T_series + pco2D_series     # Total pCO2 =  pCO2-DIC + pCO2-T\n",
    "        \n",
    "        pco2_nosocat =  pco2T_series + pco2D_unseen_series\n",
    "        \n",
    "        \n",
    "        \n",
    "        comp = xr.Dataset({\n",
    "                        'pCO2_full_DIC_recon':([\"time\",\"ylat\",\"xlon\"],pco2D_series),          #full pCO2-Residual reconstruction\n",
    "                        'pCO2_full_recon':([\"time\",\"ylat\",\"xlon\"],pco2_series),               #full pCO2 reconstruction\n",
    "                        'pCO2_DIC_unseen_recon':([\"time\",\"ylat\",\"xlon\"],pco2D_unseen_series), #unseen pCO2-Residual reconstruction\n",
    "                        'pCO2_unseen_recon':([\"time\",\"ylat\",\"xlon\"],pco2_nosocat),            #unseen pCO2 reconstruction\n",
    "                        'pCO2_T':([\"time\",\"ylat\",\"xlon\"],pco2T_series)},                      #pCO2T = temp driven component of pCO2\n",
    "                        coords={'time': (['time'],pco2T_series.time.values),\n",
    "                        'ylat': (['ylat'],pco2T_series.ylat.values),\n",
    "                        'xlon':(['xlon'],pco2T_series.xlon.values)})\n",
    "        \n",
    "        \n",
    "        if os.path.isfile(file_out):\n",
    "            os.remove(file_out)\n",
    "\n",
    "        comp.to_netcdf(f\"{file_out}\")\n",
    "        \n",
    "        del pco2T_path, pco2D_path, pco2_series, pco2T_series, pco2D_series"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cesm_play",
   "language": "python",
   "name": "cesm_play"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
